"""
æ¨¡å‹ç­–ç•¥å¯¹æ¯”å®éªŒ - ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹ vs å•ä¸€å¤šè¾“å‡ºæ¨¡å‹
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import warnings
from typing import Dict, Optional, List, Tuple
import time
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.utils.config import load_config
from src.utils.logger import setup_logger
from src.utils.metrics import calculate_multi_target_metrics, print_metrics
from src.data.multi_task_preprocessor import MultiTaskPreprocessor
from src.models.multi_task_trainer import MultiTaskTrainer
from src.models.baseline import BaselineModel
from sklearn.model_selection import KFold


def run_model_strategy_comparison(config_path: str = None, 
                                 data_dir: str = "data"):
    """
    è¿è¡Œæ¨¡å‹ç­–ç•¥å¯¹æ¯”å®éªŒ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
    """
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(
        name="model_strategy_comparison",
        level=config['experiment']['logging']['level'],
        log_dir=config['experiment']['logging']['log_dir'] if config['experiment']['logging']['save_logs'] else None,
        log_to_file=config['experiment']['logging']['save_logs']
    )
    
    logger.info("=" * 80)
    logger.info("å¼€å§‹æ¨¡å‹ç­–ç•¥å¯¹æ¯”å®éªŒ - ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹ vs å•ä¸€å¤šè¾“å‡ºæ¨¡å‹")
    logger.info("=" * 80)
    
    try:
        # 1. æ•°æ®é¢„å¤„ç†
        logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†å’Œå‡†å¤‡")
        preprocessor = MultiTaskPreprocessor(config)
        
        # åŠ è½½æ•°æ®
        datasets = preprocessor.load_competition_data(data_dir)
        
        # å‡†å¤‡å¤šä»»åŠ¡æ•°æ®ï¼ˆåªä½¿ç”¨æœ‰å®Œæ•´æ ‡ç­¾çš„æ•°æ®ï¼‰
        main_train = datasets['main_train']
        
        # è·å–æœ‰å®Œæ•´ç›®æ ‡å€¼çš„æ ·æœ¬
        target_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
        complete_mask = main_train[target_cols].notnull().all(axis=1)
        complete_data = main_train[complete_mask].reset_index(drop=True)
        
        logger.info(f"å®Œæ•´æ ‡ç­¾æ•°æ®: {len(complete_data)} æ ·æœ¬ (åŸå§‹: {len(main_train)} æ ·æœ¬)")
        
        if len(complete_data) < 50:
            logger.warning("å®Œæ•´æ ‡ç­¾æ•°æ®å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œå¯¹æ¯”")
            # å¦‚æœå®Œæ•´æ•°æ®å¤ªå°‘ï¼Œæˆ‘ä»¬ä½¿ç”¨FFVæ•°æ®ï¼ˆæœ€å¤šï¼‰è¿›è¡Œå¯¹æ¯”
            task_datasets_raw = preprocessor.prepare_task_specific_datasets(datasets)
            ffv_data = task_datasets_raw['FFV']
            
            # ä¸ºå…¶ä»–ä»»åŠ¡ç”Ÿæˆè™šæ‹Ÿæ ‡ç­¾ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
            logger.warning("ä½¿ç”¨FFVæ•°æ®ç”Ÿæˆå¤šè¾“å‡ºæ¼”ç¤ºæ•°æ®")
            demo_data = ffv_data.copy()
            np.random.seed(42)
            demo_data['Tg'] = demo_data['FFV'] * 1000 + np.random.normal(0, 10, len(demo_data))
            demo_data['Tc'] = demo_data['FFV'] * 0.5 + np.random.normal(0, 0.05, len(demo_data))
            demo_data['Density'] = 1.0 + demo_data['FFV'] * 0.3 + np.random.normal(0, 0.1, len(demo_data))
            demo_data['Rg'] = 20 + demo_data['FFV'] * 10 + np.random.normal(0, 2, len(demo_data))
            complete_data = demo_data.sample(n=min(1000, len(demo_data)), random_state=42).reset_index(drop=True)
            logger.info(f"ç”Ÿæˆæ¼”ç¤ºæ•°æ®: {len(complete_data)} æ ·æœ¬")
        
        # 2. æå–ç‰¹å¾
        logger.info("æ­¥éª¤ 2: ç‰¹å¾æå–")
        X_df, feature_names = preprocessor.extract_features_for_task(complete_data)
        X = X_df.values  # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y = complete_data[target_cols].values
        
        logger.info(f"ç‰¹å¾ç»´åº¦: {X.shape}")
        logger.info(f"ç›®æ ‡ç»´åº¦: {y.shape}")
        
        # 3. å¯¹æ¯”å®éªŒ
        logger.info("æ­¥éª¤ 3: æ¨¡å‹ç­–ç•¥å¯¹æ¯”")
        
        # ç­–ç•¥1: ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹
        logger.info("\n=== ç­–ç•¥1: ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹ ===")
        independent_results = run_independent_task_strategy(X, y, target_cols, config, logger)
        
        # ç­–ç•¥2: å•ä¸€å¤šè¾“å‡ºæ¨¡å‹
        logger.info("\n=== ç­–ç•¥2: å•ä¸€å¤šè¾“å‡ºæ¨¡å‹ ===")
        multioutput_results = run_multioutput_strategy(X, y, target_cols, config, logger)
        
        # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        logger.info("æ­¥éª¤ 4: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
        generate_strategy_comparison_report(
            independent_results, multioutput_results, target_cols, logger
        )
        
        logger.info("=" * 80)
        logger.info("æ¨¡å‹ç­–ç•¥å¯¹æ¯”å®éªŒå®Œæˆ!")
        logger.info("=" * 80)
        
        return {
            'independent': independent_results,
            'multioutput': multioutput_results
        }
        
    except Exception as e:
        logger.error(f"å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise e


def run_independent_task_strategy(X: np.ndarray, 
                                 y: np.ndarray, 
                                 target_cols: List[str],
                                 config: Dict,
                                 logger) -> Dict:
    """
    è¿è¡Œç‹¬ç«‹ä»»åŠ¡æ¨¡å‹ç­–ç•¥
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡æ•°æ®
        target_cols: ç›®æ ‡åˆ—å
        config: é…ç½®
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    start_time = time.time()
    
    # ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_splits = list(kf.split(X, y))
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(cv_splits):
        logger.info(f"ç‹¬ç«‹ä»»åŠ¡ç­–ç•¥ - ç¬¬ {fold + 1} æŠ˜")
        
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒç‹¬ç«‹æ¨¡å‹
        task_predictions = {}
        task_maes = {}
        
        for i, target_name in enumerate(target_cols):
            # è®­ç»ƒå•ä»»åŠ¡æ¨¡å‹
            model = BaselineModel(
                model_type='xgboost',
                model_params=config['models']['baseline']['xgboost'],
                use_multioutput=False
            )
            
            model.fit(X_train_fold, y_train_fold[:, i])
            pred = model.predict(X_val_fold)
            
            if len(pred.shape) > 1:
                pred = pred.ravel()
            
            task_predictions[target_name] = pred
            task_maes[target_name] = np.mean(np.abs(y_val_fold[:, i] - pred))
        
        # ç»„åˆé¢„æµ‹ç»“æœ
        y_pred_fold = np.column_stack([task_predictions[col] for col in target_cols])
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_multi_target_metrics(y_val_fold, y_pred_fold, target_cols)
        fold_results.append(metrics)
        
        # æ‰“å°æœ¬æŠ˜ç»“æœ
        logger.info(f"  ç¬¬ {fold + 1} æŠ˜å¹³å‡MAE: {metrics['overall']['mean_mae']:.6f}")
    
    # è®¡ç®—å¹³å‡ç»“æœ
    avg_metrics = average_cv_results(fold_results, target_cols)
    
    training_time = time.time() - start_time
    
    result = {
        'strategy': 'independent_tasks',
        'fold_results': fold_results,
        'average_metrics': avg_metrics,
        'training_time': training_time,
        'model_count': len(target_cols)
    }
    
    logger.info(f"ç‹¬ç«‹ä»»åŠ¡ç­–ç•¥å®Œæˆ - å¹³å‡MAE: {avg_metrics['overall']['mean_mae']:.6f} Â± {avg_metrics['overall']['mean_mae_std']:.6f}")
    
    return result


def run_multioutput_strategy(X: np.ndarray, 
                           y: np.ndarray, 
                           target_cols: List[str],
                           config: Dict,
                           logger) -> Dict:
    """
    è¿è¡Œå•ä¸€å¤šè¾“å‡ºæ¨¡å‹ç­–ç•¥
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡æ•°æ®
        target_cols: ç›®æ ‡åˆ—å
        config: é…ç½®
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœ
    """
    start_time = time.time()
    
    # ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_splits = list(kf.split(X, y))
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(cv_splits):
        logger.info(f"å¤šè¾“å‡ºç­–ç•¥ - ç¬¬ {fold + 1} æŠ˜")
        
        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # è®­ç»ƒå•ä¸€å¤šè¾“å‡ºæ¨¡å‹
        model = BaselineModel(
            model_type='xgboost',
            model_params=config['models']['baseline']['xgboost'],
            use_multioutput=True
        )
        
        model.fit(X_train_fold, y_train_fold, target_names=target_cols)
        y_pred_fold = model.predict(X_val_fold)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_multi_target_metrics(y_val_fold, y_pred_fold, target_cols)
        fold_results.append(metrics)
        
        # æ‰“å°æœ¬æŠ˜ç»“æœ
        logger.info(f"  ç¬¬ {fold + 1} æŠ˜å¹³å‡MAE: {metrics['overall']['mean_mae']:.6f}")
    
    # è®¡ç®—å¹³å‡ç»“æœ
    avg_metrics = average_cv_results(fold_results, target_cols)
    
    training_time = time.time() - start_time
    
    result = {
        'strategy': 'multioutput',
        'fold_results': fold_results,
        'average_metrics': avg_metrics,
        'training_time': training_time,
        'model_count': 1
    }
    
    logger.info(f"å¤šè¾“å‡ºç­–ç•¥å®Œæˆ - å¹³å‡MAE: {avg_metrics['overall']['mean_mae']:.6f} Â± {avg_metrics['overall']['mean_mae_std']:.6f}")
    
    return result


def average_cv_results(fold_results: List[Dict], target_cols: List[str]) -> Dict:
    """
    è®¡ç®—äº¤å‰éªŒè¯çš„å¹³å‡ç»“æœ
    
    Args:
        fold_results: æ¯æŠ˜çš„ç»“æœåˆ—è¡¨
        target_cols: ç›®æ ‡åˆ—å
        
    Returns:
        å¹³å‡ç»“æœå­—å…¸
    """
    avg_metrics = {}
    
    # ä¸ºæ¯ä¸ªç›®æ ‡è®¡ç®—å¹³å‡æŒ‡æ ‡
    for target_name in target_cols:
        target_metrics = {}
        for metric_name in ['mae', 'rmse']:
            values = [fold[target_name][metric_name] for fold in fold_results]
            target_metrics[metric_name] = np.mean(values)
            target_metrics[f'{metric_name}_std'] = np.std(values)
        avg_metrics[target_name] = target_metrics
    
    # è®¡ç®—æ€»ä½“å¹³å‡æŒ‡æ ‡
    overall_metrics = {}
    for metric_name in ['mean_mae', 'mean_rmse', 'total_mae']:
        values = [fold['overall'][metric_name] for fold in fold_results]
        overall_metrics[metric_name] = np.mean(values)
        overall_metrics[f'{metric_name}_std'] = np.std(values)
    avg_metrics['overall'] = overall_metrics
    
    return avg_metrics


def generate_strategy_comparison_report(independent_results: Dict,
                                      multioutput_results: Dict,
                                      target_cols: List[str],
                                      logger):
    """
    ç”Ÿæˆç­–ç•¥å¯¹æ¯”æŠ¥å‘Š
    
    Args:
        independent_results: ç‹¬ç«‹ä»»åŠ¡ç»“æœ
        multioutput_results: å¤šè¾“å‡ºç»“æœ
        target_cols: ç›®æ ‡åˆ—å
        logger: æ—¥å¿—å™¨
    """
    logger.info("\n" + "=" * 100)
    logger.info("æ¨¡å‹ç­–ç•¥å¯¹æ¯”æŠ¥å‘Š")
    logger.info("=" * 100)
    
    # å‡†å¤‡å¯¹æ¯”æ•°æ®
    comparison_data = []
    
    # ç‹¬ç«‹ä»»åŠ¡ç­–ç•¥
    independent_avg = independent_results['average_metrics']
    comparison_data.append({
        'strategy': 'ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹',
        'mean_mae': independent_avg['overall']['mean_mae'],
        'mean_mae_std': independent_avg['overall']['mean_mae_std'],
        'mean_rmse': independent_avg['overall']['mean_rmse'],
        'mean_rmse_std': independent_avg['overall']['mean_rmse_std'],
        'training_time': independent_results['training_time'],
        'model_count': independent_results['model_count']
    })
    
    # å¤šè¾“å‡ºç­–ç•¥
    multioutput_avg = multioutput_results['average_metrics']
    comparison_data.append({
        'strategy': 'å•ä¸€å¤šè¾“å‡ºæ¨¡å‹',
        'mean_mae': multioutput_avg['overall']['mean_mae'],
        'mean_mae_std': multioutput_avg['overall']['mean_mae_std'],
        'mean_rmse': multioutput_avg['overall']['mean_rmse'],
        'mean_rmse_std': multioutput_avg['overall']['mean_rmse_std'],
        'training_time': multioutput_results['training_time'],
        'model_count': multioutput_results['model_count']
    })
    
    # åˆ›å»ºå¯¹æ¯”DataFrame
    df = pd.DataFrame(comparison_data)
    
    # æ‰“å°æ€»ä½“å¯¹æ¯”
    logger.info(f"\næ€»ä½“æ€§èƒ½å¯¹æ¯”:")
    for _, row in df.iterrows():
        logger.info(f"  {row['strategy']:<15} "
                   f"å¹³å‡MAE: {row['mean_mae']:.6f} Â± {row['mean_mae_std']:.6f} "
                   f"è®­ç»ƒæ—¶é—´: {row['training_time']:.1f}s "
                   f"æ¨¡å‹æ•°é‡: {row['model_count']}")
    
    # è¯¦ç»†ä»»åŠ¡å¯¹æ¯”
    logger.info(f"\nå„ä»»åŠ¡è¯¦ç»†å¯¹æ¯”:")
    logger.info(f"{'ä»»åŠ¡':<10} {'ç‹¬ç«‹æ¨¡å‹MAE':<15} {'å¤šè¾“å‡ºMAE':<15} {'æ€§èƒ½å·®å¼‚':<12}")
    logger.info(f"{'-'*60}")
    
    for target_name in target_cols:
        independent_mae = independent_avg[target_name]['mae']
        multioutput_mae = multioutput_avg[target_name]['mae']
        difference = ((independent_mae - multioutput_mae) / independent_mae) * 100
        
        logger.info(f"{target_name:<10} "
                   f"{independent_mae:<15.6f} "
                   f"{multioutput_mae:<15.6f} "
                   f"{difference:>+6.2f}%")
    
    # åˆ†æå’Œå»ºè®®
    better_strategy = "ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹" if independent_avg['overall']['mean_mae'] < multioutput_avg['overall']['mean_mae'] else "å•ä¸€å¤šè¾“å‡ºæ¨¡å‹"
    mae_improvement = abs(independent_avg['overall']['mean_mae'] - multioutput_avg['overall']['mean_mae'])
    improvement_pct = (mae_improvement / max(independent_avg['overall']['mean_mae'], multioutput_avg['overall']['mean_mae'])) * 100
    
    logger.info(f"\nğŸ¯ ç­–ç•¥åˆ†æ:")
    logger.info(f"  æœ€ä½³ç­–ç•¥: {better_strategy}")
    logger.info(f"  æ€§èƒ½å·®å¼‚: {improvement_pct:.2f}%")
    
    if improvement_pct < 1.0:
        logger.info(f"  ğŸ“Š ç»“è®º: ä¸¤ç§ç­–ç•¥æ€§èƒ½ç›¸è¿‘ï¼Œå»ºè®®è€ƒè™‘ä»¥ä¸‹å› ç´ :")
        logger.info(f"    - ç‹¬ç«‹æ¨¡å‹: æ›´çµæ´»ï¼Œå¯é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œä½†å¤æ‚åº¦é«˜")
        logger.info(f"    - å¤šè¾“å‡ºæ¨¡å‹: æ›´ç®€æ´ï¼Œè®­ç»ƒå¿«é€Ÿï¼Œä½†è°ƒä¼˜å—é™")
    else:
        if better_strategy == "ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹":
            logger.info(f"  ğŸ“Š ç»“è®º: ç‹¬ç«‹ä»»åŠ¡æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œé€‚åˆå½“å‰å¤šä»»åŠ¡åœºæ™¯")
        else:
            logger.info(f"  ğŸ“Š ç»“è®º: å•ä¸€å¤šè¾“å‡ºæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œå»ºè®®é‡‡ç”¨")
    
    # ä¿å­˜ç»“æœ
    results_dir = Path("results/model_strategy_comparison")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    df.to_csv(results_dir / "strategy_comparison_summary.csv", index=False)
    logger.info(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜: {results_dir / 'strategy_comparison_summary.csv'}")
    logger.info("=" * 100)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œæ¨¡å‹ç­–ç•¥å¯¹æ¯”å®éªŒ')
    parser.add_argument('--config', type=str, default='configs/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='data', help='æ•°æ®ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    # è¿è¡Œå®éªŒ
    results = run_model_strategy_comparison(
        config_path=args.config,
        data_dir=args.data_dir
    )
    
    return results


if __name__ == "__main__":
    main()