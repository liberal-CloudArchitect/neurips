"""
ç‰¹å¾å¯¹æ¯”å®éªŒè„šæœ¬ - æµ‹è¯•MACCS Keysçš„æ€§èƒ½æå‡
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import warnings
from typing import Dict, Optional, List
import time
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.utils.config import load_config, update_config
from src.utils.logger import setup_logger
from src.data.multi_task_preprocessor import MultiTaskPreprocessor
from src.models.multi_task_trainer import MultiTaskTrainer


def run_feature_comparison_experiment(config_path: str = None, 
                                    data_dir: str = "data",
                                    test_task: str = "FFV"):
    """
    è¿è¡Œç‰¹å¾å¯¹æ¯”å®éªŒ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        test_task: ç”¨äºæµ‹è¯•çš„ä»»åŠ¡åç§°ï¼ˆé€‰æ‹©æ•°æ®é‡å¤§çš„ä»»åŠ¡ï¼‰
    """
    # åŠ è½½é…ç½®
    base_config = load_config(config_path)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(
        name="feature_comparison",
        level=base_config['experiment']['logging']['level'],
        log_dir=base_config['experiment']['logging']['log_dir'] if base_config['experiment']['logging']['save_logs'] else None,
        log_to_file=base_config['experiment']['logging']['save_logs']
    )
    
    logger.info("=" * 80)
    logger.info("å¼€å§‹ç‰¹å¾å¯¹æ¯”å®éªŒ - æµ‹è¯•MACCS Keysæ€§èƒ½æå‡")
    logger.info("=" * 80)
    
    # å®šä¹‰ç‰¹å¾ç»„åˆå®éªŒé…ç½®
    feature_configs = [
        {
            "name": "Morgan_Only",
            "morgan_enabled": True,
            "maccs_enabled": False,
            "descriptors_enabled": False
        },
        {
            "name": "Morgan_Descriptors",
            "morgan_enabled": True,
            "maccs_enabled": False,
            "descriptors_enabled": True
        },
        {
            "name": "Morgan_MACCS",
            "morgan_enabled": True,
            "maccs_enabled": True,
            "descriptors_enabled": False
        },
        {
            "name": "All_Features",
            "morgan_enabled": True,
            "maccs_enabled": True,
            "descriptors_enabled": True
        },
        {
            "name": "MACCS_Only",
            "morgan_enabled": False,
            "maccs_enabled": True,
            "descriptors_enabled": False
        }
    ]
    
    results = []
    
    try:
        for i, feature_config in enumerate(feature_configs):
            logger.info(f"\n{'='*60}")
            logger.info(f"å®éªŒ {i+1}/{len(feature_configs)}: {feature_config['name']}")
            logger.info(f"{'='*60}")
            
            # åˆ›å»ºç‰¹å®šçš„é…ç½®
            config = update_config(base_config, {
                'features': {
                    'morgan_fingerprint': {
                        'enabled': feature_config['morgan_enabled']
                    },
                    'maccs_keys': {
                        'enabled': feature_config['maccs_enabled']
                    },
                    'rdkit_descriptors': {
                        'use_2d': feature_config['descriptors_enabled']
                    }
                }
            })
            
            # è¿è¡Œå•æ¬¡å®éªŒ
            result = run_single_feature_experiment(
                config, data_dir, test_task, feature_config['name'], logger
            )
            
            if result:
                results.append(result)
                logger.info(f"å®éªŒå®Œæˆ - {feature_config['name']}: MAE = {result['mae']:.6f}")
            else:
                logger.error(f"å®éªŒå¤±è´¥ - {feature_config['name']}")
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        if results:
            generate_comparison_report(results, logger)
        
        logger.info("=" * 80)
        logger.info("ç‰¹å¾å¯¹æ¯”å®éªŒå®Œæˆ!")
        logger.info("=" * 80)
        
        return results
        
    except Exception as e:
        logger.error(f"å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise e


def run_single_feature_experiment(config: Dict, 
                                 data_dir: str, 
                                 test_task: str,
                                 experiment_name: str,
                                 logger) -> Optional[Dict]:
    """
    è¿è¡Œå•ä¸ªç‰¹å¾é…ç½®çš„å®éªŒ
    
    Args:
        config: å®éªŒé…ç½®
        data_dir: æ•°æ®ç›®å½•
        test_task: æµ‹è¯•ä»»åŠ¡
        experiment_name: å®éªŒåç§°
        logger: æ—¥å¿—å™¨
        
    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    try:
        start_time = time.time()
        
        # 1. æ•°æ®é¢„å¤„ç†
        logger.info(f"  æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç† - {experiment_name}")
        preprocessor = MultiTaskPreprocessor(config)
        
        # åŠ è½½æ•°æ®
        datasets = preprocessor.load_competition_data(data_dir)
        
        # å‡†å¤‡ä»»åŠ¡ç‰¹å®šæ•°æ®é›†
        task_datasets_raw = preprocessor.prepare_task_specific_datasets(datasets)
        
        # ä»…å¤„ç†æµ‹è¯•ä»»åŠ¡
        if test_task not in task_datasets_raw:
            logger.error(f"æµ‹è¯•ä»»åŠ¡ {test_task} ä¸å­˜åœ¨")
            return None
        
        # å‡†å¤‡å•ä¸ªä»»åŠ¡çš„æ•°æ®
        task_dataset = preprocessor.prepare_single_task_dataset(test_task, test_size=0.2)
        
        # è®°å½•ç‰¹å¾ç»´åº¦
        feature_dim = task_dataset['X_train'].shape[1]
        logger.info(f"  ç‰¹å¾ç»´åº¦: {feature_dim}")
        
        # 2. æ¨¡å‹è®­ç»ƒ
        logger.info(f"  æ­¥éª¤ 2: æ¨¡å‹è®­ç»ƒ - {experiment_name}")
        trainer = MultiTaskTrainer(config)
        
        # åªè®­ç»ƒæµ‹è¯•ä»»åŠ¡
        task_datasets = {test_task: task_dataset}
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆåªç”¨XGBooståŠ å¿«é€Ÿåº¦ï¼‰
        all_results = trainer.train_all_tasks(
            task_datasets=task_datasets,
            model_types=['xgboost']  # åªä½¿ç”¨XGBooståŠ å¿«å®éªŒé€Ÿåº¦
        )
        
        # 3. æå–ç»“æœ
        if test_task in trainer.task_models:
            model_info = trainer.task_models[test_task]
            metrics = model_info['metrics']
            
            training_time = time.time() - start_time
            
            result = {
                'experiment_name': experiment_name,
                'task': test_task,
                'feature_dim': feature_dim,
                'mae': metrics['mae'],
                'mae_std': metrics['mae_std'],
                'rmse': metrics['rmse'],
                'rmse_std': metrics['rmse_std'],
                'training_time': training_time,
                'samples': len(task_datasets_raw[test_task])
            }
            
            return result
        else:
            logger.error(f"è®­ç»ƒå¤±è´¥ - {experiment_name}")
            return None
            
    except Exception as e:
        logger.error(f"å•ä¸ªå®éªŒå¤±è´¥ - {experiment_name}: {str(e)}")
        return None


def generate_comparison_report(results: List[Dict], logger):
    """
    ç”Ÿæˆç‰¹å¾å¯¹æ¯”æŠ¥å‘Š
    
    Args:
        results: å®éªŒç»“æœåˆ—è¡¨
        logger: æ—¥å¿—å™¨
    """
    logger.info("\n" + "=" * 100)
    logger.info("ç‰¹å¾å¯¹æ¯”å®éªŒæŠ¥å‘Š")
    logger.info("=" * 100)
    
    # åˆ›å»ºç»“æœDataFrame
    df = pd.DataFrame(results)
    df = df.sort_values('mae')
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    best_result = df.iloc[0]
    baseline_result = df[df['experiment_name'] == 'Morgan_Only'].iloc[0] if len(df[df['experiment_name'] == 'Morgan_Only']) > 0 else None
    
    logger.info(f"\nä»»åŠ¡: {best_result['task']}")
    logger.info(f"æ ·æœ¬æ•°: {best_result['samples']}")
    
    logger.info(f"\nç‰¹å¾é…ç½®æ€§èƒ½æ’å:")
    for i, (_, row) in enumerate(df.iterrows()):
        logger.info(f"  {i+1}. {row['experiment_name']:<20} "
                   f"MAE: {row['mae']:.6f} Â± {row['mae_std']:.6f} "
                   f"ç‰¹å¾ç»´åº¦: {row['feature_dim']:>4d} "
                   f"æ—¶é—´: {row['training_time']:.1f}s")
    
    # æ€§èƒ½æå‡åˆ†æ
    if baseline_result is not None:
        logger.info(f"\næ€§èƒ½æå‡åˆ†æ (vs Morgan_OnlyåŸºçº¿):")
        baseline_mae = baseline_result['mae']
        
        for _, row in df.iterrows():
            if row['experiment_name'] != 'Morgan_Only':
                improvement = (baseline_mae - row['mae']) / baseline_mae * 100
                logger.info(f"  {row['experiment_name']:<20} "
                           f"æå‡: {improvement:+.2f}% "
                           f"(MAE: {baseline_mae:.6f} â†’ {row['mae']:.6f})")
    
    # MACCS Keysä»·å€¼åˆ†æ
    morgan_only = df[df['experiment_name'] == 'Morgan_Only']
    morgan_maccs = df[df['experiment_name'] == 'Morgan_MACCS']
    
    if len(morgan_only) > 0 and len(morgan_maccs) > 0:
        mae_morgan = morgan_only.iloc[0]['mae']
        mae_morgan_maccs = morgan_maccs.iloc[0]['mae']
        maccs_improvement = (mae_morgan - mae_morgan_maccs) / mae_morgan * 100
        
        logger.info(f"\nğŸ¯ MACCS Keysä»·å€¼åˆ†æ:")
        logger.info(f"  Morgan Only:      MAE = {mae_morgan:.6f}")
        logger.info(f"  Morgan + MACCS:   MAE = {mae_morgan_maccs:.6f}")
        logger.info(f"  MACCS Keysæå‡:   {maccs_improvement:+.2f}%")
        
        if maccs_improvement > 0:
            logger.info(f"  âœ… MACCS Keysæœ‰æ•ˆï¼å»ºè®®å¯ç”¨")
        else:
            logger.info(f"  âŒ MACCS Keysæ— æ˜æ˜¾æå‡")
    
    # ä¿å­˜ç»“æœ
    results_dir = Path("results/feature_comparison")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    df.to_csv(results_dir / "feature_comparison_results.csv", index=False)
    logger.info(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜: {results_dir / 'feature_comparison_results.csv'}")
    logger.info("=" * 100)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œç‰¹å¾å¯¹æ¯”å®éªŒ')
    parser.add_argument('--config', type=str, default='configs/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='data', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--task', type=str, default='FFV', help='æµ‹è¯•ä»»åŠ¡åç§°')
    
    args = parser.parse_args()
    
    # è¿è¡Œå®éªŒ
    results = run_feature_comparison_experiment(
        config_path=args.config,
        data_dir=args.data_dir,
        test_task=args.task
    )
    
    return results


if __name__ == "__main__":
    main()