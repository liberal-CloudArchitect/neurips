#!/usr/bin/env python
"""
Transformeræ¨¡å—è®¾ç½®æµ‹è¯•è„šæœ¬

æµ‹è¯•ç¬¬å››é˜¶æ®µTransformerå¼€å‘çš„å…³é”®ç»„ä»¶ï¼š
1. SMILESåˆ†è¯å™¨åŠŸèƒ½
2. Transformeræ¨¡å‹åˆ›å»ºå’Œå‰å‘ä¼ æ’­
3. é…ç½®åŠ è½½
4. åŸºç¡€è®­ç»ƒæµç¨‹

ä½¿ç”¨æ–¹æ³•:
    python test_transformer_setup.py
"""

import sys
import warnings
from pathlib import Path
import torch
import numpy as np
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

warnings.filterwarnings('ignore')

# æµ‹è¯•é¢œè‰²è¾“å‡º
def print_success(message):
    print(f"âœ… {message}")

def print_error(message):
    print(f"âŒ {message}")

def print_warning(message):
    print(f"âš ï¸ {message}")

def print_info(message):
    print(f"ğŸ” {message}")


def test_library_imports():
    """æµ‹è¯•åº“å¯¼å…¥"""
    print_info("æµ‹è¯•åº“å¯¼å…¥...")
    
    try:
        # æ ¸å¿ƒåº“
        import torch
        print_success(f"PyTorch: {torch.__version__}")
        
        # Transformersåº“ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        try:
            import transformers
            print_success(f"Transformers: {transformers.__version__}")
        except ImportError:
            print_warning("Transformersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨è‡ªå®šä¹‰å®ç°")
        
        # åŒ–å­¦åº“
        try:
            import rdkit
            print_success("RDKit: å¯¼å…¥æˆåŠŸ")
        except ImportError:
            print_error("RDKitæœªå®‰è£…ï¼Œè¿™æ˜¯å¿…éœ€çš„åº“")
            return False
        
        # æ•°æ®å¤„ç†åº“
        import numpy
        import pandas
        print_success("NumPyå’ŒPandas: å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print_error(f"åº“å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_smiles_tokenizer():
    """æµ‹è¯•SMILESåˆ†è¯å™¨"""
    print_info("æµ‹è¯•SMILESåˆ†è¯å™¨...")
    
    try:
        from src.data.smiles_tokenizer import SMILESTokenizer, create_smiles_tokenizer
        
        # æµ‹è¯•SMILES
        test_smiles = [
            "CCO",  # ä¹™é†‡
            "CC(C)O",  # å¼‚ä¸™é†‡
            "c1ccccc1",  # è‹¯
            "CC(=O)NC1=CC=C(C=C1)O",  # å¯¹ä¹™é…°æ°¨åŸºé…š
            "CN1CCC[C@H]1c2cccnc2",  # å°¼å¤ä¸
            "Invalid_SMILES"  # æ— æ•ˆSMILES
        ]
        
        print(f"  æµ‹è¯•SMILES: {test_smiles[:4]}...")
        
        # åˆ›å»ºåˆ†è¯å™¨
        tokenizer = create_smiles_tokenizer(test_smiles, vocab_size=200, max_length=128)
        
        print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
        print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {tokenizer.max_length}")
        
        # æµ‹è¯•ç¼–ç å’Œè§£ç 
        test_smiles_sample = "CCO"
        encoded = tokenizer.encode(test_smiles_sample, return_tensors='pt')
        decoded = tokenizer.decode(encoded['input_ids'])
        
        print(f"  æµ‹è¯•ç¼–ç /è§£ç :")
        print(f"    åŸå§‹: {test_smiles_sample}")
        print(f"    è§£ç : {decoded}")
        print(f"    Tokenæ•°: {len(encoded['tokens'])}")
        print(f"    è¾“å…¥ç»´åº¦: {encoded['input_ids'].shape}")
        print(f"    æ³¨æ„åŠ›æ©ç ç»´åº¦: {encoded['attention_mask'].shape}")
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        batch_encoded = tokenizer.batch_encode(test_smiles[:3], return_tensors='pt')
        print(f"  æ‰¹é‡ç¼–ç ç»´åº¦: {batch_encoded['input_ids'].shape}")
        
        # æµ‹è¯•ç‰¹æ®Štoken
        special_tokens = tokenizer.get_special_tokens_dict()
        print(f"  ç‰¹æ®Štoken: {special_tokens}")
        
        print_success("SMILESåˆ†è¯å™¨æµ‹è¯•é€šè¿‡")
        return tokenizer
        
    except Exception as e:
        print_error(f"SMILESåˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_transformer_models():
    """æµ‹è¯•Transformeræ¨¡å‹"""
    print_info("æµ‹è¯•Transformeræ¨¡å‹...")
    
    try:
        from src.models.transformer import (
            create_transformer_model, 
            count_transformer_parameters,
            MultiTaskTransformerPredictor,
            BertBasedSMILESPredictor
        )
        
        # æ¨¡å‹é…ç½®
        vocab_size = 200
        config = {
            'd_model': 128,
            'nhead': 4,
            'num_layers': 2,
            'dim_feedforward': 256,
            'max_seq_length': 64,
            'dropout': 0.1,
            'pooling_strategy': 'cls',
            'task_names': ['Density', 'Tc']
        }
        
        # æµ‹è¯•è‡ªå®šä¹‰Transformer
        print("  æµ‹è¯•è‡ªå®šä¹‰Transformeræ¨¡å‹...")
        custom_model = create_transformer_model('custom', vocab_size, config)
        param_count = count_transformer_parameters(custom_model)
        print(f"    è‡ªå®šä¹‰æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_length = 32
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
        attention_mask = torch.ones_like(input_ids)
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        custom_model.eval()
        with torch.no_grad():
            outputs = custom_model(input_ids, attention_mask)
            print(f"    è‡ªå®šä¹‰æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {list(outputs['predictions'].keys())}")
            for task, pred in outputs['predictions'].items():
                print(f"      {task}: {pred.shape}")
        
        # æµ‹è¯•BERT-basedæ¨¡å‹ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼Œå› ä¸ºæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼‰
        print("  æµ‹è¯•BERT-basedæ¨¡å‹...")
        try:
            bert_config = config.copy()
            bert_config['pretrained_model_name'] = 'DeepChem/ChemBERTa-77M-MLM'
            bert_model = create_transformer_model('bert_based', vocab_size, bert_config)
            print(f"    BERTæ¨¡å‹å‚æ•°æ•°é‡: {count_transformer_parameters(bert_model):,}")
            print_success("BERT-basedæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print_warning(f"BERT-basedæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼ˆé¢„æœŸçš„ï¼‰: {e}")
        
        print_success("Transformeræ¨¡å‹æµ‹è¯•é€šè¿‡")
        return custom_model
        
    except Exception as e:
        print_error(f"Transformeræ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print_info("æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        from src.utils.config import load_config
        
        config_path = "configs/config.yaml"
        config = load_config(config_path)
        
        # æ£€æŸ¥Transformeré…ç½®
        if 'transformer' in config.get('models', {}):
            transformer_config = config['models']['transformer']
            print(f"  Transformeré…ç½®åŠ è½½æˆåŠŸ:")
            print(f"    æ¨¡å‹ç»´åº¦: {transformer_config.get('d_model', 'N/A')}")
            print(f"    æ³¨æ„åŠ›å¤´æ•°: {transformer_config.get('nhead', 'N/A')}")
            print(f"    å±‚æ•°: {transformer_config.get('num_layers', 'N/A')}")
            print(f"    è¯æ±‡è¡¨å¤§å°: {transformer_config.get('vocab_size', 'N/A')}")
            print(f"    æœ€å¤§åºåˆ—é•¿åº¦: {transformer_config.get('max_seq_length', 'N/A')}")
            print(f"    é¢„è®­ç»ƒæ¨¡å‹: {transformer_config.get('pretrained_model_name', 'N/A')}")
        else:
            print_warning("é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°Transformeré…ç½®")
        
        print_success("é…ç½®æµ‹è¯•é€šè¿‡")
        return config
        
    except Exception as e:
        print_error(f"é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_training_integration():
    """æµ‹è¯•è®­ç»ƒé›†æˆ"""
    print_info("æµ‹è¯•è®­ç»ƒé›†æˆ...")
    
    try:
        from src.models.transformer_trainer import TransformerTrainer, SMILESDataset, collate_fn
        from torch.utils.data import DataLoader
        
        # æ¨¡æ‹Ÿé…ç½®
        config = {
            'models': {
                'transformer': {
                    'd_model': 64,
                    'nhead': 4,
                    'num_layers': 2,
                    'vocab_size': 100,
                    'max_seq_length': 32,
                    'task_names': ['Density']
                }
            },
            'training': {
                'device': 'cpu',
                'batch_size': 2,
                'epochs': 2,
                'learning_rate': 1e-3
            },
            'data': {
                'validation': {
                    'n_splits': 2,
                    'random_state': 42
                }
            }
        }
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = TransformerTrainer(config)
        
        # æµ‹è¯•æ•°æ®å‡†å¤‡
        test_smiles = ["CCO", "CC(C)O", "c1ccccc1", "CC(=O)O"]
        test_targets = np.array([1.0, 2.0, 3.0, 4.0])
        
        # å‡†å¤‡åˆ†è¯å™¨
        tokenizer = trainer.prepare_tokenizer(test_smiles)
        print(f"    åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
        
        # æµ‹è¯•æ•°æ®é›†
        dataset = SMILESDataset(
            smiles_list=test_smiles,
            targets=test_targets.reshape(-1, 1),
            tokenizer=tokenizer,
            target_names=['Density']
        )
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        for batch in dataloader:
            print(f"    æ‰¹æ¬¡é”®: {list(batch.keys())}")
            print(f"    è¾“å…¥ç»´åº¦: {batch['input_ids'].shape}")
            print(f"    ç›®æ ‡ç»´åº¦: {batch['targets']['Density'].shape}")
            break
        
        print_success("è®­ç»ƒé›†æˆæµ‹è¯•é€šè¿‡")
        return trainer
        
    except Exception as e:
        print_error(f"è®­ç»ƒé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_end_to_end_mini_training():
    """æµ‹è¯•ç«¯åˆ°ç«¯çš„è¿·ä½ è®­ç»ƒ"""
    print_info("æµ‹è¯•ç«¯åˆ°ç«¯è¿·ä½ è®­ç»ƒ...")
    
    try:
        from src.models.transformer_trainer import TransformerTrainer
        
        # ç®€åŒ–é…ç½®
        config = {
            'models': {
                'transformer': {
                    'd_model': 64,
                    'nhead': 4,
                    'num_layers': 2,
                    'vocab_size': 100,
                    'max_seq_length': 32,
                    'task_names': ['test_task']
                }
            },
            'training': {
                'device': 'cpu',
                'batch_size': 2,
                'epochs': 2,
                'learning_rate': 1e-3,
                'weight_decay': 1e-5,
                'scheduler_patience': 1,
                'early_stopping': {'patience': 1},
                'loss': {'type': 'mse'}
            },
            'data': {
                'validation': {
                    'n_splits': 2,
                    'random_state': 42
                }
            }
        }
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        train_smiles = ["CCO", "CC(C)O", "c1ccccc1", "CC(=O)O"] * 2  # 8ä¸ªæ ·æœ¬
        train_targets = np.random.normal(0, 1, len(train_smiles))
        val_smiles = ["CCN", "CC(C)N"]
        val_targets = np.random.normal(0, 1, len(val_smiles))
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = TransformerTrainer(config)
        
        # å‡†å¤‡åˆ†è¯å™¨
        all_smiles = train_smiles + val_smiles
        trainer.prepare_tokenizer(all_smiles)
        
        # å°è¯•å•ä¸ªepochçš„è®­ç»ƒ
        print("    å¼€å§‹è¿·ä½ è®­ç»ƒ...")
        model, history = trainer.train_single_task_model(
            train_smiles=train_smiles,
            train_targets=train_targets,
            val_smiles=val_smiles,
            val_targets=val_targets,
            task_name='test_task',
            model_type='custom'
        )
        
        print(f"    è®­ç»ƒå®Œæˆï¼Œå†å²é•¿åº¦: {len(history['train_loss'])}")
        print(f"    æœ€ç»ˆè®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.6f}")
        print(f"    æœ€ç»ˆéªŒè¯MAE: {history['val_mae'][-1]:.6f}")
        
        # æµ‹è¯•é¢„æµ‹
        test_smiles = ["CCO", "CCN"]
        predictions = trainer.predict(model, test_smiles, 'test_task')
        print(f"    æµ‹è¯•é¢„æµ‹: {predictions}")
        
        print_success("ç«¯åˆ°ç«¯è¿·ä½ è®­ç»ƒæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print_error(f"ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Transformeræ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    test_results = {
        'åº“å¯¼å…¥': False,
        'SMILESåˆ†è¯å™¨': False,
        'Transformeræ¨¡å‹': False,
        'é…ç½®åŠ è½½': False,
        'è®­ç»ƒé›†æˆ': False,
        'ç«¯åˆ°ç«¯è®­ç»ƒ': False
    }
    
    # 1. æµ‹è¯•åº“å¯¼å…¥
    test_results['åº“å¯¼å…¥'] = test_library_imports()
    print()
    
    if not test_results['åº“å¯¼å…¥']:
        print_error("åŸºç¡€åº“å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
        return
    
    # 2. æµ‹è¯•SMILESåˆ†è¯å™¨
    tokenizer = test_smiles_tokenizer()
    test_results['SMILESåˆ†è¯å™¨'] = tokenizer is not None
    print()
    
    # 3. æµ‹è¯•Transformeræ¨¡å‹
    model = test_transformer_models()
    test_results['Transformeræ¨¡å‹'] = model is not None
    print()
    
    # 4. æµ‹è¯•é…ç½®åŠ è½½
    config = test_config_loading()
    test_results['é…ç½®åŠ è½½'] = config is not None
    print()
    
    # 5. æµ‹è¯•è®­ç»ƒé›†æˆ
    trainer = test_training_integration()
    test_results['è®­ç»ƒé›†æˆ'] = trainer is not None
    print()
    
    # 6. æµ‹è¯•ç«¯åˆ°ç«¯è®­ç»ƒï¼ˆå¦‚æœå‰é¢éƒ½é€šè¿‡ï¼‰
    if all([test_results['SMILESåˆ†è¯å™¨'], test_results['Transformeræ¨¡å‹']]):
        test_results['ç«¯åˆ°ç«¯è®­ç»ƒ'] = test_end_to_end_mini_training()
    else:
        print_warning("è·³è¿‡ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•ï¼ˆä¾èµ–ç»„ä»¶æµ‹è¯•å¤±è´¥ï¼‰")
    print()
    
    # æµ‹è¯•æ€»ç»“
    print("=" * 50)
    print("ğŸ æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    
    for test_name, passed in test_results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    passed_count = sum(test_results.values())
    total_count = len(test_results)
    
    print(f"\næ€»ä½“ç»“æœ: {passed_count}/{total_count} æµ‹è¯•é€šè¿‡")
    
    if passed_count == total_count:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Transformeræ¨¡å—å‡†å¤‡å°±ç»ª")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥ï¼š")
        print("1. è¿è¡Œå¿«é€Ÿæµ‹è¯•: python src/experiments/transformer_experiment.py --test_tokenizer")
        print("2. è¿è¡Œå°è§„æ¨¡è®­ç»ƒ: python src/experiments/transformer_experiment.py --max_samples 50")
        print("3. è¿è¡Œå®Œæ•´è®­ç»ƒ: python src/experiments/transformer_experiment.py")
        print("4. æ¨¡å‹å¯¹æ¯”: python src/experiments/transformer_experiment.py --compare")
    else:
        print("âš ï¸ å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œè¯·æ£€æŸ¥å¹¶è§£å†³é—®é¢˜")
        print("\nğŸ”§ å¸¸è§é—®é¢˜è§£å†³:")
        print("1. å¦‚æœTransformersåº“å¯¼å…¥å¤±è´¥:")
        print("   pip install transformers")
        print("2. å¦‚æœRDKitå¯¼å…¥å¤±è´¥:")
        print("   pip install rdkit-pypi")
        print("3. å¦‚æœæ¨¡å‹åˆ›å»ºå¤±è´¥:")
        print("   æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§")
        print("4. å¦‚æœå†…å­˜ä¸è¶³:")
        print("   å‡å°æ¨¡å‹å°ºå¯¸æˆ–æ‰¹æ¬¡å¤§å°")


if __name__ == "__main__":
    main()